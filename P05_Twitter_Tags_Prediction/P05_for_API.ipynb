{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "247b9f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/demid/opt/anaconda3/lib/python3.9/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim import corpora\n",
    "import spacy, logging, warnings\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score,\\\n",
    "                            recall_score, f1_score, jaccard_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c76389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/demid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/demid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82072ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('code')\n",
    "stopwords.append('new')\n",
    "stopwords.append('use')\n",
    "stopwords.append('code')\n",
    "stopwords.append('pre')\n",
    "stopwords.append('would')\n",
    "stopwords.append('get')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780294a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_sw(text):\n",
    "    words = text.lower().split()\n",
    "    stop_free_words = [word for word in words if word not in stopwords] \n",
    "    stop_free_text = \" \".join(stop_free_words)\n",
    "    return stop_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2421dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/demid/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('/Users/demid/Downloads/QueryResults(1).csv', sep=',', engine='python', error_bad_lines=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c24fda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(map(lambda x: set(re.findall(\"<(.*?)>\", x)), df1['Tags']))\n",
    "tags = sorted(set.union(*tags))\n",
    "df1['Tags'] = list(map(lambda x: re.findall(\"<(.*?)>\", x), df1['Tags']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2149f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = []\n",
    "\n",
    "for i in range(0, 50000):\n",
    "    for j in range(len(df1['Tags'].iloc[i])):\n",
    "        all_tags.append(df1['Tags'].iloc[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f40abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CounterVariable = Counter(all_tags)\n",
    "most_occur = CounterVariable.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0206ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "centtags = []\n",
    "for i in range(0, 100):\n",
    "    centtags.append(most_occur[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76c18130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tags_filtered'] = [[t for t in Tags if t in centtags] for Tags in df1['Tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f33099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496289cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe664063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2a5ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"clean_title\"] = df1[\"Title\"].apply(lambda s: ' '.join(re.sub(\"(w+://S+)\", \" \", s).split()))\n",
    "df1[\"clean_title\"] = df1[\"clean_title\"].apply(lambda s: ' '.join(re.sub(\"[.,!?):;(-='...@#_>]\", \" \", s).split()))\n",
    "df1[\"clean_title\"] = df1[\"clean_title\"].apply(lambda s: ' '.join(re.sub(r'[0-9]+', '', s).split()))\n",
    "df1['clean_title'] = df1['clean_title'].apply(lambda  s: rem_sw(s))\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "df1['clean_title'] = df1['clean_title'].apply(lambda  x:  tokenizer.tokenize(x))  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df1[\"clean_title\"] = df1[\"clean_title\"].apply(lambda tokens: [lemmatizer.lemmatize(token, pos='v') for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13ca391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"clean_body\"] = df1[\"Body\"].apply(lambda s: ' '.join(re.sub(\"<p>\", \" \", s).split()))\n",
    "df1[\"clean_body\"] = df1[\"clean_body\"].apply(lambda s: ' '.join(re.sub(\"(w+://S+)\", \" \", s).split()))\n",
    "df1[\"clean_body\"] = df1[\"clean_body\"].apply(lambda s: ' '.join(re.sub(\"[.,!?):;(-='...@#_>]\", \" \", s).split()))\n",
    "df1[\"clean_body\"] = df1[\"clean_body\"].apply(lambda s: ' '.join(re.sub(r'[0-9]+', '', s).split()))\n",
    "df1['clean_body'] = df1['clean_body'].apply(lambda  s: rem_sw(s))\n",
    "df1['clean_body'] = df1['clean_body'].apply(lambda  x:  tokenizer.tokenize(x))       \n",
    "df1[\"clean_body\"] = df1[\"clean_body\"].apply(lambda tokens: [lemmatizer.lemmatize(token, pos='v') for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7679808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.drop(df1[['Title', 'Body', 'Tags', 'Id', 'Score', 'ViewCount', 'FavoriteCount',\n",
    "       'AnswerCount']], axis=1)\n",
    "df2['text'] = df2['clean_title']   + df2['clean_body']\n",
    "df3 = df2.drop(df2[['clean_title', 'clean_body']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f6ee985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['text']=[\" \".join(text) for text in df3['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28f49791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags_filtered</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[java, json, spring, spring-mvc]</td>\n",
       "      <td>spring json request body map java pojo use spr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[android]</td>\n",
       "      <td>communicate activity service localservice andr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[shell]</td>\n",
       "      <td>set email address mailx command work kornshell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[visual-studio]</td>\n",
       "      <td>ankhsvn versus visualsvn currently ankhsvn int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[c, performance, optimization]</td>\n",
       "      <td>performance difference c performance differenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      tags_filtered  \\\n",
       "0  [java, json, spring, spring-mvc]   \n",
       "1                         [android]   \n",
       "2                           [shell]   \n",
       "3                   [visual-studio]   \n",
       "4    [c, performance, optimization]   \n",
       "\n",
       "                                                text  \n",
       "0  spring json request body map java pojo use spr...  \n",
       "1  communicate activity service localservice andr...  \n",
       "2  set email address mailx command work kornshell...  \n",
       "3  ankhsvn versus visualsvn currently ankhsvn int...  \n",
       "4  performance difference c performance differenc...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "98668e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1772dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "tag_mlb = mlb.fit_transform(df3['tags_filtered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ffad591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('classify',\n",
      "                 MultiOutputClassifier(estimator=RandomForestClassifier()))])\n"
     ]
    }
   ],
   "source": [
    "classifier = MultiOutputClassifier(RandomForestClassifier())\n",
    "\n",
    "clf = Pipeline([('classify', classifier)])\n",
    "\n",
    "print (clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5665ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0, X_test_0, y_train, y_test = train_test_split(\n",
    "    df3['text'], tag_mlb, test_size=0.2, random_state=11)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_0)\n",
    "X_test = vectorizer.transform(X_test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "225e2292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f1a4b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.164\n",
      "Precision :  0.5056733333333333\n",
      "Recall :  0.48128999999999994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/demid/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/demid/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/demid/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score :  0.4611915873015873\n",
      "Jaccard : 0.37978345238095235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/demid/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_predict = dt.predict(X_test)\n",
    "print(\"Accuracy : \", accuracy_score(y_test, y_predict))\n",
    "print(\"Precision : \", precision_score(y_test, y_predict, average='samples'))\n",
    "print(\"Recall : \", recall_score(y_test, y_predict, average='samples'))\n",
    "print(\"F1 Score : \", f1_score(y_test, y_predict, average='samples'))\n",
    "print(\"Jaccard :\", jaccard_score(y_test, y_predict, average='samples'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36616807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('classify',\n",
      "                 MultiOutputClassifier(estimator=DecisionTreeClassifier()))])\n"
     ]
    }
   ],
   "source": [
    "classifier1 = MultiOutputClassifier(DecisionTreeClassifier())\n",
    "\n",
    "clf1 = Pipeline([('classify', classifier1)])\n",
    "\n",
    "print (clf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.fit(X_train, y_train)\n",
    "print(clf1.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat1 = clf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5dd668c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.78.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /Users/demid/opt/anaconda3/lib/python3.9/site-packages (from fastapi) (1.8.2)\n",
      "Collecting starlette==0.19.1\n",
      "  Downloading starlette-0.19.1-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
      "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /Users/demid/opt/anaconda3/lib/python3.9/site-packages (from starlette==0.19.1->fastapi) (3.10.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/demid/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/demid/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (3.2)\n",
      "Installing collected packages: anyio, starlette, fastapi\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 2.2.0\n",
      "    Uninstalling anyio-2.2.0:\n",
      "      Successfully uninstalled anyio-2.2.0\n",
      "Successfully installed anyio-3.6.1 fastapi-0.78.0 starlette-0.19.1\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install fastapi \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3068e",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba5a5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "236a8f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2a26ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
     ]
    }
   ],
   "source": [
    "w = w2v(\n",
    "    df3['text'],\n",
    "    min_count=3,  \n",
    "    sg = 1,       \n",
    "    window=7      \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08eb40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df3['text'].to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "386e0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_size=300\n",
    "w2v_window=5\n",
    "w2v_min_count=1\n",
    "w2v_epochs=100\n",
    "maxlen = 24 # adapt to length of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4072931",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window,\n",
    "                                                vector_size=w2v_size,\n",
    "                                                seed=42,\n",
    "                                                workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87d5e18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e29cd051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(634083528, 681145400)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "97b3007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vectors = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9a00544",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_words = model_vectors.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b7d6285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 139214\n",
      "Word2Vec trained\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff6fdbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Tokenizer ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9659b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "685e148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 139215\n"
     ]
    }
   ],
   "source": [
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28bab63",
   "metadata": {},
   "source": [
    "### Création de la matrice d'embedding¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b33b92c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Embedding matrix ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Create Embedding matrix ...\")\n",
    "w2v_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i=0\n",
    "j=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4582d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "228e2c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding rate :  1.0\n",
      "Embedding matrix: (139215, 300)\n"
     ]
    }
   ],
   "source": [
    "word_rate = np.round(j/i,4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f490aa",
   "metadata": {},
   "source": [
    "### Création du modèle d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "20c94a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 08:37:06.676980: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 24)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 24, 300)           41764500  \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 300)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,764,500\n",
      "Trainable params: 41,764,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle\n",
    "\n",
    "input=Input(shape=(len(x_sentences),maxlen),dtype='float64')\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')  \n",
    "word_embedding=Embedding(input_dim=vocab_size,\n",
    "                         output_dim=w2v_size,\n",
    "                         weights = [embedding_matrix],\n",
    "                         input_length=maxlen)(word_input)\n",
    "word_vec=GlobalAveragePooling1D()(word_embedding)  \n",
    "embed_model = Model([word_input],word_vec)\n",
    "\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d95e8e",
   "metadata": {},
   "source": [
    "## Exécution du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05ac11dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 1s 646us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embed_model.predict(x_sentences)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdba042",
   "metadata": {},
   "source": [
    "### Use of the Embedding matrix for Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339a53f",
   "metadata": {},
   "source": [
    "    Split the data into text (X) and labels (Y)\n",
    "    Preprocess X\n",
    "    Create a word embedding matrix from X\n",
    "    Create a tensor input from X\n",
    "    Train a deep learning model using the tensor inputs and labels (Y)\n",
    "    Make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cffcfb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95126432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into text (X) and labels (Y)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    df3['text'], tag_mlb, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "af93e24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f3275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a4c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f86113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ede716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd2e2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess X_train1\n",
    "\n",
    "#sentences = X_train1.to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in X_train1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e1aee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:sorting after vectors have been allocated is expensive & error-prone\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4744340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(505743991, 543106600)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "abb6b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vectors = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d0a2e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_words = model_vectors.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8bc507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 120038\n",
      "Word2Vec trained\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "90e9948a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Tokenizer ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "210a13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sentences = pad_sequences(tokenizer.texts_to_sequences(sentences),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5f2c514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 120039\n"
     ]
    }
   ],
   "source": [
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "98d1c86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Embedding matrix ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Create Embedding matrix ...\")\n",
    "w2v_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i=0\n",
    "j=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1f795dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9b05088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding rate :  1.0\n",
      "Embedding matrix: (120039, 300)\n"
     ]
    }
   ],
   "source": [
    "word_rate = np.round(j/i,4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "862ed1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 24)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 24, 300)           36011700  \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 300)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,011,700\n",
      "Trainable params: 36,011,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle d'embedding\n",
    "\n",
    "input=Input(shape=(len(x_sentences),maxlen),dtype='float64')\n",
    "word_input=Input(shape=(maxlen,),dtype='float64')  \n",
    "word_embedding=Embedding(input_dim=vocab_size,\n",
    "                         output_dim=w2v_size,\n",
    "                         weights = [embedding_matrix],\n",
    "                         input_length=maxlen)(word_input)\n",
    "word_vec=GlobalAveragePooling1D()(word_embedding)  \n",
    "embed_model = Model([word_input],word_vec)\n",
    "\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7fa17e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 1s 655us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40000, 300)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings1 = embed_model.predict(x_sentences)\n",
    "embeddings1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749fc15",
   "metadata": {},
   "source": [
    "#### I do the same for X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b1a6abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences2 = [gensim.utils.simple_preprocess(text) for text in X_test1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "089617a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.keyedvectors:sorting after vectors have been allocated is expensive & error-prone\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f9570d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128319014, 138038800)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences2, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "415e9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vectors = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1f8951ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_words = model_vectors.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0702ea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 47528\n",
      "Word2Vec trained\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b6c52e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Tokenizer ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "febb85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sentences2 = pad_sequences(tokenizer.texts_to_sequences(sentences2),\n",
    "                                                     maxlen=maxlen,\n",
    "                                                     padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c793023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 47529\n"
     ]
    }
   ],
   "source": [
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "80654045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Embedding matrix ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Create Embedding matrix ...\")\n",
    "w2v_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i=0\n",
    "j=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2484c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c3595037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding rate :  1.0\n",
      "Embedding matrix: (47529, 300)\n"
     ]
    }
   ],
   "source": [
    "word_rate = np.round(j/i,4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5a293064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings2 = embed_model.predict(x_sentences2)\n",
    "embeddings2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a43707a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d91ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6896e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a7533a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtW = DecisionTreeClassifier()\n",
    "dtW.fit(embeddings1, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "23c8dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.0109\n",
      "Precision :  0.04906\n",
      "Recall :  0.046518333333333335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/demid/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/demid/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/demid/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score :  0.04381912698412698\n",
      "Jaccard : 0.03210797619047618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/demid/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_predict1 = dtW.predict(embeddings2)\n",
    "print(\"Accuracy : \", accuracy_score(y_test1, y_predict1))\n",
    "print(\"Precision : \", precision_score(y_test1, y_predict1, average='samples'))\n",
    "print(\"Recall : \", recall_score(y_test1, y_predict1, average='samples'))\n",
    "print(\"F1 Score : \", f1_score(y_test1, y_predict1, average='samples'))\n",
    "print(\"Jaccard :\", jaccard_score(y_test1, y_predict1, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbaebb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "242ee151",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "55ea8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import metrics as kmetrics\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Bert\n",
    "import os\n",
    "import transformers\n",
    "from transformers import *\n",
    "\n",
    "os.environ[\"TF_KERAS\"]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6d6ed39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "331cd3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "07685b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "2.9.1\n",
      "Num GPUs Available:  0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1c3bc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de préparation des sentences\n",
    "def bert_inp_fct(sentences, bert_tokenizer, max_length) :\n",
    "    input_ids=[]\n",
    "    token_type_ids = []\n",
    "    attention_mask=[]\n",
    "    bert_inp_tot = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens = True,\n",
    "                                              max_length = max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask = True, \n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "    \n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0], \n",
    "                             bert_inp['token_type_ids'][0], \n",
    "                             bert_inp['attention_mask'][0]))\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "    \n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot\n",
    "    \n",
    "\n",
    "# Fonction de création des features\n",
    "def feature_BERT_fct(model, model_type, sentences, max_length, b_size, mode='HF') :\n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size) :\n",
    "        idx = step*batch_size\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx+batch_size], \n",
    "                                                                      bert_tokenizer, max_length)\n",
    "        \n",
    "        if mode=='HF' :    # Bert HuggingFace\n",
    "            outputs = model.predict([input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        if mode=='TFhub' : # Bert Tensorflow Hub\n",
    "            text_preprocessed = {\"input_word_ids\" : input_ids, \n",
    "                                 \"input_mask\" : attention_mask, \n",
    "                                 \"input_type_ids\" : token_type_ids}\n",
    "            outputs = model(text_preprocessed)\n",
    "            last_hidden_states = outputs['sequence_output']\n",
    "             \n",
    "        if step ==0 :\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "            last_hidden_states_tot_0 = last_hidden_states\n",
    "        else :\n",
    "            last_hidden_states_tot = np.concatenate((last_hidden_states_tot,last_hidden_states))\n",
    "    \n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "    \n",
    "    time2 = np.round(time.time() - time1,0)\n",
    "    print(\"temps traitement : \", time2)\n",
    "     \n",
    "    return features_bert, last_hidden_states_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aedeff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886d88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f1fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5f680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
